{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-UvAo672RCU2xdxQxWU0mfeYngO_Y_iX","timestamp":1729966678583}],"authorship_tag":"ABX9TyO7AAiOzkcpR2uFvUiO/t28"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Spacy NER model for PI redaction**\n","\n","The following notebook contains the initial usage of the spacy NER model to evaluate whether or not it is a good performer for the project."],"metadata":{"id":"XbOrtRxo72Ml"}},{"cell_type":"markdown","source":["Step 1: imports"],"metadata":{"id":"y4HwqHTd8FUe"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Xgn0X4G4as4","executionInfo":{"status":"ok","timestamp":1729966802441,"user_tz":240,"elapsed":26356,"user":{"displayName":"Emma Teff","userId":"08710051236758747688"}},"outputId":"8ed8735b-c8e9-4c23-b08a-9560d3fb2bf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Trafilatura\n","  Downloading trafilatura-1.12.2-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from Trafilatura) (2024.8.30)\n","Collecting courlan>=1.2.0 (from Trafilatura)\n","  Downloading courlan-1.3.1-py3-none-any.whl.metadata (17 kB)\n","Collecting htmldate>=1.8.1 (from Trafilatura)\n","  Downloading htmldate-1.9.1-py3-none-any.whl.metadata (10 kB)\n","Collecting justext>=3.0.1 (from Trafilatura)\n","  Downloading jusText-3.0.1-py2.py3-none-any.whl.metadata (6.9 kB)\n","Collecting lxml>=5.2.2 (from Trafilatura)\n","  Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from Trafilatura) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from Trafilatura) (2.2.3)\n","Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from courlan>=1.2.0->Trafilatura) (2.16.0)\n","Collecting tld>=0.13 (from courlan>=1.2.0->Trafilatura)\n","  Downloading tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n","Collecting dateparser>=1.1.2 (from htmldate>=1.8.1->Trafilatura)\n","  Downloading dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from htmldate>=1.8.1->Trafilatura) (2.8.2)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.8.1->Trafilatura) (2024.2)\n","Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.8.1->Trafilatura) (2024.9.11)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.8.1->Trafilatura) (5.2)\n","Collecting lxml-html-clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->Trafilatura)\n","  Downloading lxml_html_clean-0.3.1-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->htmldate>=1.8.1->Trafilatura) (1.16.0)\n","Downloading trafilatura-1.12.2-py3-none-any.whl (132 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.2/132.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading courlan-1.3.1-py3-none-any.whl (33 kB)\n","Downloading htmldate-1.9.1-py3-none-any.whl (31 kB)\n","Downloading jusText-3.0.1-py2.py3-none-any.whl (837 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tld-0.13-py2.py3-none-any.whl (263 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lxml_html_clean-0.3.1-py3-none-any.whl (13 kB)\n","Installing collected packages: tld, lxml, lxml-html-clean, dateparser, courlan, htmldate, justext, Trafilatura\n","  Attempting uninstall: lxml\n","    Found existing installation: lxml 4.9.4\n","    Uninstalling lxml-4.9.4:\n","      Successfully uninstalled lxml-4.9.4\n","Successfully installed Trafilatura-1.12.2 courlan-1.3.1 dateparser-1.2.0 htmldate-1.9.1 justext-3.0.1 lxml-5.3.0 lxml-html-clean-0.3.1 tld-0.13\n"]}],"source":["!pip install Trafilatura\n","import spacy\n","from trafilatura.core import *\n","from trafilatura import fetch_url\n","import os"]},{"cell_type":"markdown","source":["Step 2: load model"],"metadata":{"id":"y7v0uLSt8X8X"}},{"cell_type":"code","source":["# Load spacy NER model\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"b0BnYq0W7zlM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 3: mount subset of GovDocs dataset"],"metadata":{"id":"ZypGwRqI8qzj"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","govdocs_dir ='/content/drive/My Drive/ECE570/govdocs_testingdata'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSIRmi-c-do7","executionInfo":{"status":"ok","timestamp":1729967176190,"user_tz":240,"elapsed":22600,"user":{"displayName":"Emma Teff","userId":"08710051236758747688"}},"outputId":"e31a29b5-fb7d-4dad-baf0-06c9af836719"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["Step 4: create a list of the html files from the dataset"],"metadata":{"id":"jf_8j_d0-d-g"}},{"cell_type":"code","source":[],"metadata":{"id":"InA4ohvV4Ni9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to identify and read HTML files from the dataset\n","def find_html_files(directory):\n","    html_files = []\n","    for root, dirs, files in os.walk(directory):\n","        for file in files:\n","            # Check if the file has an HTML extension\n","            if file.endswith(\".html\") or file.endswith(\".htm\"):\n","                html_files.append(os.path.join(root, file))\n","    return html_files\n","\n","# Find HTML files in the dataset\n","html_files = find_html_files(govdocs_dir)"],"metadata":{"id":"ARgNdu3w8rqv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Step 5: for each html file, extract content with Trafilatura and use the model to remove PII"],"metadata":{"id":"rHQ91YKI8cLy"}},{"cell_type":"code","source":["# Function to detect and redact personal information using Named Entity Recognition\n","def remove_personal_info(text):\n","    doc = nlp(text)\n","    pi_entities = ['PERSON', 'EMAIL']\n","    redacted_text = text\n","\n","    instances = 0\n","    for ent in doc.ents:\n","        if ent.label_ in pi_entities:\n","            redacted_text = redacted_text.replace(ent.text, '[REDACTED]')\n","            instances += 1\n","\n","    return redacted_text, instances\n","\n","# Process the HTML content with Trafilatura's extract() function\n","# detect and remove PII with the spacy model\n","def process_html(html_content):\n","    text = extract(html_content, favor_recall=True)\n","    if text:\n","        # Apply PI redaction\n","        redacted_text, instances = remove_personal_info(text)\n","        return redacted_text, instances\n","    else:\n","        return None\n","\n","# Function to read the content of an HTML file\n","def read_html_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n","        return f.read()\n","\n","# for each HTML file, extract content and detect and remove PII\n","if html_files:\n","    i = 0\n","    total_instances = 0\n","    for file in html_files:\n","        html_content = read_html_file(file)\n","        redacted_content, instances = process_html(html_content)\n","        total_instances += instances\n","        # if redacted_content:\n","        #     try:\n","        #         with open(f'./spacy_PI_filtered_content/test{i}.txt', 'w', encoding='utf-8') as fp:\n","        #             fp.write(redacted_content)\n","        #         i += 1\n","        #     except Exception as e:\n","        #         print(f\"Error writing to file: {e}\")\n","        # else:\n","        #     print(f'Failed to extract text from {file}.')\n","    print(\"Total instances of PI detected in GovDocs subset: \" + str(total_instances))\n","\n","# test a specific case, the ECE 404 homepage\n","ece404 = '/content/drive/My Drive/ECE570/ECE404.html'\n","with open(ece404, 'r', encoding='utf-8') as fp:\n","    ece404html_content = fp.read()\n","text = extract(ece404html_content, favor_recall=True) # favor_precision=True will cut out noise, favor_recall=True will keep more in\n","redacted, count = remove_personal_info(str(text))\n","print(\"Total instances of PI detected in ECE404 homepage: \" + str(count))\n","print(str(redacted))\n"],"metadata":{"id":"EZkxlbw14eBp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729967362767,"user_tz":240,"elapsed":118846,"user":{"displayName":"Emma Teff","userId":"08710051236758747688"}},"outputId":"655fc956-f7b6-4dbc-a95f-19c05d023aa8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total instances of PI detected in GovDocs subset: 19758\n","Total instances of PI detected in ECE404 homepage: 7\n","Instructor: [REDACTED]Professor, ECE\n","E-mail: [REDACTED] (You must place the string 'ece404' in the subject line to get past your instructor's notorious spam filter)\n","Graduate TAs:\n","[REDACTED]\n","-\n","E-mail: kashyap9 (at purdue dot edu)\n","[REDACTED]E-mail: wang3450 (at purdue dot edu)\n","[REDACTED]\n","-\n","E-mail: [REDACTED] (at purdue dot edu)\n","Lecture Location and Time\n","-\n","TuTh: 6:00 PM - 7:15 PM, PHYS 112\n","Course Description\n","-\n","Beyond question, computer and network security has emerged as one of\n","the most important subjects of study in modern times. Even the minutest\n","details of our lives now depend on our computers and networks working\n","with our trust that the information that is private to us will not fall\n","in the hands of those with ill intent. The two major components of\n","computer and network security are cryptography and what is known as\n","systems-oriented security. For a good education in computer and network\n","security, you have no choice but to learn them both. That, then, is the\n","goal of this course: To provide a balanced introduction to both cryptography\n","and the systems-oriented issues. The systems-oriented issues we will cover\n","in this course include Denial-of-Service attacks, DNS [REDACTED] attacks,\n","Buffer Overflow attacks, Dictionary attacks, attacks with viruses, worms, and\n","Trojans, etc.\n","Homework and Exam Credit:\n","-\n","You will earn 50% of your credit from homework assignments (including programming assignments) and 50% from three exams.\n","NOTE: All the homework assignments and announcements will be posted on Brightspace for Spring 2024. Do not use the links below.\n","Course Materials:\n"]}]}]}